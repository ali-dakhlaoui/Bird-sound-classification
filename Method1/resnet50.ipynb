{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6b5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchlibrosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edaccaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/core/errors.py:144: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/home/hamroua/.local/lib/python3.6/site-packages/resampy/interpn.py:114: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
      "  _resample_loop_p(x, t_out, interp_win, interp_delta, num_table, scale, y)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e707efb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'flight call']</td>\n",
       "      <td>12.3910</td>\n",
       "      <td>-1.4930</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>08:00</td>\n",
       "      <td>https://www.xeno-canto.org/125458</td>\n",
       "      <td>afrsil1/XC125458.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>['houspa', 'redava', 'zebdov']</td>\n",
       "      <td>['call']</td>\n",
       "      <td>19.8801</td>\n",
       "      <td>-155.7254</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Dan Lane</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>08:30</td>\n",
       "      <td>https://www.xeno-canto.org/175522</td>\n",
       "      <td>afrsil1/XC175522.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'song']</td>\n",
       "      <td>16.2901</td>\n",
       "      <td>-16.0321</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:30</td>\n",
       "      <td>https://www.xeno-canto.org/177993</td>\n",
       "      <td>afrsil1/XC177993.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['alarm call', 'call']</td>\n",
       "      <td>17.0922</td>\n",
       "      <td>54.2958</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Oscar Campbell</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:00</td>\n",
       "      <td>https://www.xeno-canto.org/205893</td>\n",
       "      <td>afrsil1/XC205893.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['flight call']</td>\n",
       "      <td>21.4581</td>\n",
       "      <td>-157.7252</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Ross Gallardy</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16:30</td>\n",
       "      <td>https://www.xeno-canto.org/207431</td>\n",
       "      <td>afrsil1/XC207431.ogg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label                secondary_labels                     type  \\\n",
       "0       afrsil1                              []  ['call', 'flight call']   \n",
       "1       afrsil1  ['houspa', 'redava', 'zebdov']                 ['call']   \n",
       "2       afrsil1                              []         ['call', 'song']   \n",
       "3       afrsil1                              []   ['alarm call', 'call']   \n",
       "4       afrsil1                              []          ['flight call']   \n",
       "\n",
       "   latitude  longitude  scientific_name         common_name          author  \\\n",
       "0   12.3910    -1.4930  Euodice cantans  African Silverbill       Bram Piot   \n",
       "1   19.8801  -155.7254  Euodice cantans  African Silverbill        Dan Lane   \n",
       "2   16.2901   -16.0321  Euodice cantans  African Silverbill       Bram Piot   \n",
       "3   17.0922    54.2958  Euodice cantans  African Silverbill  Oscar Campbell   \n",
       "4   21.4581  -157.7252  Euodice cantans  African Silverbill   Ross Gallardy   \n",
       "\n",
       "                                             license  rating   time  \\\n",
       "0  Creative Commons Attribution-NonCommercial-Sha...     2.5  08:00   \n",
       "1  Creative Commons Attribution-NonCommercial-Sha...     3.5  08:30   \n",
       "2  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:30   \n",
       "3  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:00   \n",
       "4  Creative Commons Attribution-NonCommercial-Sha...     3.0  16:30   \n",
       "\n",
       "                                 url              filename  \n",
       "0  https://www.xeno-canto.org/125458  afrsil1/XC125458.ogg  \n",
       "1  https://www.xeno-canto.org/175522  afrsil1/XC175522.ogg  \n",
       "2  https://www.xeno-canto.org/177993  afrsil1/XC177993.ogg  \n",
       "3  https://www.xeno-canto.org/205893  afrsil1/XC205893.ogg  \n",
       "4  https://www.xeno-canto.org/207431  afrsil1/XC207431.ogg  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Data:\n",
    "TRAIN_DIR = './train_audio/'\n",
    "IMAGES_DIR = './images/'\n",
    "SAMPLE_RATE = 32000\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "# Data processing:\n",
    "N_FFT = 2048\n",
    "HOP_LEN = 512\n",
    "WIN_FUNC = 'hann'\n",
    "N_MELS = 224\n",
    "F_MIN = 0\n",
    "F_MAX = SAMPLE_RATE / 2\n",
    "\n",
    "# Learning process:\n",
    "NAME_MODEL_0 = \"model_resnet50_inst.h5\"\n",
    "NAME_MODEL_0_PIC = 'model_resnet50_pic.png'\n",
    "NAME_MODEL_0_CHECKPOINT = 'model_resnet50_cp.ckpt'\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "N_CHANNELS = 3\n",
    "EPOCHS = 100\n",
    "CALL_BACKS = [tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=NAME_MODEL_0_CHECKPOINT,\n",
    "    save_weights_only=True,\n",
    "    verbose=0\n",
    ")]\n",
    "\n",
    "train_metadata = pd.read_csv('./train_metadata.csv')\n",
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad90c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load work classes:\n",
    "with open('./scored_birds.json', 'r') as f:\n",
    "    valid_classes = json.load(f)\n",
    "\n",
    "primary_labels = train_metadata.primary_label\n",
    "\n",
    "# Encode labels:\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "labels = encoder.fit_transform(primary_labels)\n",
    "labels = np.uint8(labels)\n",
    "\n",
    "NUM_CLASSES = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50bae2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144843 files belonging to 152 classes.\n",
      "Using 115875 files for training.\n",
      "Found 144843 files belonging to 152 classes.\n",
      "Using 28968 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Make a dataset containing the training spectrograms\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VAL_SIZE,\n",
    "    directory=IMAGES_DIR,\n",
    "    shuffle=True,\n",
    "    color_mode='rgb',\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    subset=\"training\",\n",
    "    label_mode='categorical',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Make a dataset containing the validation spectrogram\n",
    "valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VAL_SIZE,\n",
    "    directory=IMAGES_DIR,\n",
    "    shuffle=True,\n",
    "    color_mode='rgb',\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    subset=\"validation\",\n",
    "    label_mode='categorical',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f1f3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare our datasets for modelling\n",
    "def prepare(ds, augment=False):\n",
    "    # Define our one transformation\n",
    "    rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1./255)])\n",
    "    flip_and_rotate = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "    ])\n",
    "    \n",
    "    # Apply rescale to both datasets and augmentation only to training\n",
    "    ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n",
    "    if augment: ds = ds.map(lambda x, y: (flip_and_rotate(x, training=True), y))\n",
    "    return ds\n",
    "\n",
    "train_dataset = prepare(train_dataset, augment=False)\n",
    "valid_dataset = prepare(valid_dataset, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ba2642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from  tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def getModel():\n",
    "    resnet50_input = Input(shape = (224, 224, 3), name = 'Image_input')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Get back the convolutional part of a Resnet network trained on ImageNet\n",
    "    model_resnet50_conv = ResNet50(weights= 'imagenet', include_top=False, input_shape= (224,224,3))\n",
    "\n",
    "   #Use the generated model \n",
    "\n",
    "\n",
    "    output_resnet50_conv= model_resnet50_conv(resnet50_input)\n",
    "\n",
    "    #Add the fully-connected layers \n",
    "\n",
    "    x = Flatten(name='flatten')(output_resnet50_conv)\n",
    "    x = Dense(152, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    resnet50_pretrained = Model(resnet50_input,x)\n",
    "    # vgg16_pretrained.summary()\n",
    "\n",
    "    # Compile CNN model\n",
    "    sgd = tf.keras.optimizers.SGD(lr = 0.001)\n",
    "    resnet50_pretrained.compile(loss='categorical_crossentropy',optimizer = sgd,metrics=['accuracy'])\n",
    "\n",
    "    return resnet50_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0495da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotMetrics(history):\n",
    "    metrics = list()\n",
    "    for key, value in history.history.items():\n",
    "        metrics.append(key)\n",
    "        \n",
    "    for i in range(int(len(metrics) / 2)):\n",
    "        plt.figure(figsize=(24, 6))\n",
    "        plt.plot(history.history[metrics[i]], c =\"darkblue\")\n",
    "        plt.plot(history.history[metrics[i + int(len(metrics) / 2)]], c =\"crimson\")\n",
    "        plt.legend([\"Train\", \"Validation\"])\n",
    "        plt.title(\"Model\" + metrics[i])\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(metrics[i])\n",
    "        plt.grid(True, alpha = 0.2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "543ac92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import image \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdfdef4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamroua/.local/lib/python3.6/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3622/3622 [==============================] - 9404s 3s/step - loss: 2.1356 - accuracy: 0.5296 - val_loss: 1.6359 - val_accuracy: 0.6113\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.63588, saving model to resnet50_weights.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamroua/.local/lib/python3.6/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "3622/3622 [==============================] - 9252s 3s/step - loss: 0.9356 - accuracy: 0.7685 - val_loss: 1.6942 - val_accuracy: 0.6144\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.63588\n",
      "Epoch 3/100\n",
      "3622/3622 [==============================] - 9259s 3s/step - loss: 0.4866 - accuracy: 0.8819 - val_loss: 1.5051 - val_accuracy: 0.6570\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.63588 to 1.50509, saving model to resnet50_weights.hdf5\n",
      "Epoch 4/100\n",
      "3622/3622 [==============================] - 9351s 3s/step - loss: 0.2527 - accuracy: 0.9447 - val_loss: 1.3159 - val_accuracy: 0.7064\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.50509 to 1.31592, saving model to resnet50_weights.hdf5\n",
      "Epoch 5/100\n",
      "3622/3622 [==============================] - 10173s 3s/step - loss: 0.1422 - accuracy: 0.9723 - val_loss: 3.2074 - val_accuracy: 0.5332\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.31592\n",
      "Epoch 6/100\n",
      "3622/3622 [==============================] - 8934s 2s/step - loss: 0.0916 - accuracy: 0.9838 - val_loss: 4.8249 - val_accuracy: 0.3279\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.31592\n",
      "Epoch 7/100\n",
      "3622/3622 [==============================] - 8380s 2s/step - loss: 0.0623 - accuracy: 0.9900 - val_loss: 1.4098 - val_accuracy: 0.7189\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.31592\n",
      "Epoch 8/100\n",
      "3622/3622 [==============================] - 10025s 3s/step - loss: 0.0424 - accuracy: 0.9941 - val_loss: 1.9707 - val_accuracy: 0.6395\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.31592\n",
      "Epoch 9/100\n",
      "3622/3622 [==============================] - 10707s 3s/step - loss: 0.0324 - accuracy: 0.9959 - val_loss: 1.8538 - val_accuracy: 0.6538\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.31592\n",
      "Epoch 10/100\n",
      "3622/3622 [==============================] - 9926s 3s/step - loss: 0.0258 - accuracy: 0.9971 - val_loss: 1.6537 - val_accuracy: 0.6845\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.31592\n",
      "Epoch 11/100\n",
      "3622/3622 [==============================] - 9908s 3s/step - loss: 0.0215 - accuracy: 0.9979 - val_loss: 1.3377 - val_accuracy: 0.7376\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.31592\n",
      "Epoch 12/100\n",
      "3622/3622 [==============================] - 9795s 3s/step - loss: 0.0179 - accuracy: 0.9982 - val_loss: 1.5243 - val_accuracy: 0.7109\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.31592\n",
      "Epoch 13/100\n",
      "3622/3622 [==============================] - 10145s 3s/step - loss: 0.0170 - accuracy: 0.9984 - val_loss: 1.3334 - val_accuracy: 0.7386\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.31592\n",
      "Epoch 14/100\n",
      "3622/3622 [==============================] - 9538s 3s/step - loss: 0.0151 - accuracy: 0.9985 - val_loss: 1.3811 - val_accuracy: 0.7318\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.31592\n",
      "Epoch 15/100\n",
      "3622/3622 [==============================] - 9521s 3s/step - loss: 0.0142 - accuracy: 0.9986 - val_loss: 1.3442 - val_accuracy: 0.7383\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.31592\n",
      "Epoch 16/100\n",
      "3622/3622 [==============================] - 9668s 3s/step - loss: 0.0128 - accuracy: 0.9987 - val_loss: 1.3488 - val_accuracy: 0.7402\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.31592\n",
      "Epoch 17/100\n",
      "3622/3622 [==============================] - 9466s 3s/step - loss: 0.0122 - accuracy: 0.9988 - val_loss: 1.3503 - val_accuracy: 0.7402\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.31592\n",
      "Epoch 18/100\n",
      "3622/3622 [==============================] - 9621s 3s/step - loss: 0.0120 - accuracy: 0.9988 - val_loss: 1.5932 - val_accuracy: 0.7100\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.31592\n",
      "Epoch 19/100\n",
      "3622/3622 [==============================] - 8476s 2s/step - loss: 0.0120 - accuracy: 0.9987 - val_loss: 1.4467 - val_accuracy: 0.7292\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.31592\n",
      "Epoch 20/100\n",
      "3622/3622 [==============================] - 8364s 2s/step - loss: 0.0114 - accuracy: 0.9988 - val_loss: 1.3648 - val_accuracy: 0.7405\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.31592\n",
      "Epoch 21/100\n",
      "3622/3622 [==============================] - 9051s 2s/step - loss: 0.0105 - accuracy: 0.9989 - val_loss: 1.3572 - val_accuracy: 0.7417\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.31592\n",
      "Epoch 22/100\n",
      "3622/3622 [==============================] - 9245s 3s/step - loss: 0.0102 - accuracy: 0.9989 - val_loss: 1.3989 - val_accuracy: 0.7358\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.31592\n",
      "Epoch 23/100\n",
      "3622/3622 [==============================] - 9332s 3s/step - loss: 0.0102 - accuracy: 0.9989 - val_loss: 1.3589 - val_accuracy: 0.7423\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.31592\n",
      "Epoch 24/100\n",
      "3622/3622 [==============================] - 9255s 3s/step - loss: 0.0097 - accuracy: 0.9989 - val_loss: 1.3858 - val_accuracy: 0.7386\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.31592\n",
      "Epoch 25/100\n",
      "3622/3622 [==============================] - 9179s 3s/step - loss: 0.0097 - accuracy: 0.9989 - val_loss: 1.3868 - val_accuracy: 0.7376\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.31592\n",
      "Epoch 26/100\n",
      "3622/3622 [==============================] - 9233s 3s/step - loss: 0.0095 - accuracy: 0.9989 - val_loss: 1.3597 - val_accuracy: 0.7416\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.31592\n",
      "Epoch 27/100\n",
      "3622/3622 [==============================] - 9612s 3s/step - loss: 0.0093 - accuracy: 0.9989 - val_loss: 1.3927 - val_accuracy: 0.7378\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.31592\n",
      "Epoch 28/100\n",
      "3622/3622 [==============================] - 9636s 3s/step - loss: 0.0094 - accuracy: 0.9990 - val_loss: 1.3661 - val_accuracy: 0.7421\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.31592\n",
      "Epoch 29/100\n",
      "3622/3622 [==============================] - 9232s 3s/step - loss: 0.0091 - accuracy: 0.9989 - val_loss: 1.3654 - val_accuracy: 0.7425\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.31592\n",
      "Epoch 30/100\n",
      "3622/3622 [==============================] - 9362s 3s/step - loss: 0.0089 - accuracy: 0.9990 - val_loss: 1.3686 - val_accuracy: 0.7419\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.31592\n",
      "Epoch 31/100\n",
      "3622/3622 [==============================] - 9153s 3s/step - loss: 0.0087 - accuracy: 0.9990 - val_loss: 1.3695 - val_accuracy: 0.7423\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.31592\n",
      "Epoch 32/100\n",
      "3622/3622 [==============================] - 9125s 3s/step - loss: 0.0089 - accuracy: 0.9989 - val_loss: 1.3664 - val_accuracy: 0.7430\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.31592\n",
      "Epoch 33/100\n",
      "3622/3622 [==============================] - 9246s 3s/step - loss: 0.0086 - accuracy: 0.9990 - val_loss: 1.3662 - val_accuracy: 0.7440\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.31592\n",
      "Epoch 34/100\n",
      "3622/3622 [==============================] - 9916s 3s/step - loss: 0.0086 - accuracy: 0.9989 - val_loss: 1.3683 - val_accuracy: 0.7438\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.31592\n",
      "Epoch 35/100\n",
      "3622/3622 [==============================] - 9694s 3s/step - loss: 0.0086 - accuracy: 0.9989 - val_loss: 1.3813 - val_accuracy: 0.7416\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.31592\n",
      "Epoch 36/100\n",
      "3622/3622 [==============================] - 9596s 3s/step - loss: 0.0087 - accuracy: 0.9990 - val_loss: 1.3684 - val_accuracy: 0.7441\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.31592\n",
      "Epoch 37/100\n",
      "3622/3622 [==============================] - 9708s 3s/step - loss: 0.0085 - accuracy: 0.9989 - val_loss: 1.3730 - val_accuracy: 0.7437\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.31592\n",
      "Epoch 38/100\n",
      "3622/3622 [==============================] - 9678s 3s/step - loss: 0.0085 - accuracy: 0.9989 - val_loss: 1.3673 - val_accuracy: 0.7442\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.31592\n",
      "Epoch 00038: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,LearningRateScheduler\n",
    "import math\n",
    "checkpointer = ModelCheckpoint('resnet50_weights.hdf5', verbose=1, save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='accuracy', patience=7, verbose=1)\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "        math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "\n",
    "model = getModel()\n",
    "tf.keras.utils.plot_model(model, NAME_MODEL_0_PIC, show_shapes=True)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[checkpointer, earlystopper,lrate]\n",
    ")\n",
    "model.save(NAME_MODEL_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8679f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
