{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "599bb457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchlibrosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d8059e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/core/errors.py:144: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/home/hamroua/.local/lib/python3.6/site-packages/resampy/interpn.py:114: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
      "  _resample_loop_p(x, t_out, interp_win, interp_delta, num_table, scale, y)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "711948e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'flight call']</td>\n",
       "      <td>12.3910</td>\n",
       "      <td>-1.4930</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>08:00</td>\n",
       "      <td>https://www.xeno-canto.org/125458</td>\n",
       "      <td>afrsil1/XC125458.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>['houspa', 'redava', 'zebdov']</td>\n",
       "      <td>['call']</td>\n",
       "      <td>19.8801</td>\n",
       "      <td>-155.7254</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Dan Lane</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>08:30</td>\n",
       "      <td>https://www.xeno-canto.org/175522</td>\n",
       "      <td>afrsil1/XC175522.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'song']</td>\n",
       "      <td>16.2901</td>\n",
       "      <td>-16.0321</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:30</td>\n",
       "      <td>https://www.xeno-canto.org/177993</td>\n",
       "      <td>afrsil1/XC177993.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['alarm call', 'call']</td>\n",
       "      <td>17.0922</td>\n",
       "      <td>54.2958</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Oscar Campbell</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:00</td>\n",
       "      <td>https://www.xeno-canto.org/205893</td>\n",
       "      <td>afrsil1/XC205893.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['flight call']</td>\n",
       "      <td>21.4581</td>\n",
       "      <td>-157.7252</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Ross Gallardy</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16:30</td>\n",
       "      <td>https://www.xeno-canto.org/207431</td>\n",
       "      <td>afrsil1/XC207431.ogg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label                secondary_labels                     type  \\\n",
       "0       afrsil1                              []  ['call', 'flight call']   \n",
       "1       afrsil1  ['houspa', 'redava', 'zebdov']                 ['call']   \n",
       "2       afrsil1                              []         ['call', 'song']   \n",
       "3       afrsil1                              []   ['alarm call', 'call']   \n",
       "4       afrsil1                              []          ['flight call']   \n",
       "\n",
       "   latitude  longitude  scientific_name         common_name          author  \\\n",
       "0   12.3910    -1.4930  Euodice cantans  African Silverbill       Bram Piot   \n",
       "1   19.8801  -155.7254  Euodice cantans  African Silverbill        Dan Lane   \n",
       "2   16.2901   -16.0321  Euodice cantans  African Silverbill       Bram Piot   \n",
       "3   17.0922    54.2958  Euodice cantans  African Silverbill  Oscar Campbell   \n",
       "4   21.4581  -157.7252  Euodice cantans  African Silverbill   Ross Gallardy   \n",
       "\n",
       "                                             license  rating   time  \\\n",
       "0  Creative Commons Attribution-NonCommercial-Sha...     2.5  08:00   \n",
       "1  Creative Commons Attribution-NonCommercial-Sha...     3.5  08:30   \n",
       "2  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:30   \n",
       "3  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:00   \n",
       "4  Creative Commons Attribution-NonCommercial-Sha...     3.0  16:30   \n",
       "\n",
       "                                 url              filename  \n",
       "0  https://www.xeno-canto.org/125458  afrsil1/XC125458.ogg  \n",
       "1  https://www.xeno-canto.org/175522  afrsil1/XC175522.ogg  \n",
       "2  https://www.xeno-canto.org/177993  afrsil1/XC177993.ogg  \n",
       "3  https://www.xeno-canto.org/205893  afrsil1/XC205893.ogg  \n",
       "4  https://www.xeno-canto.org/207431  afrsil1/XC207431.ogg  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Data:\n",
    "TRAIN_DIR = './train_audio/'\n",
    "IMAGES_DIR = './images/'\n",
    "SAMPLE_RATE = 32000\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "# Data processing:\n",
    "N_FFT = 2048\n",
    "HOP_LEN = 512\n",
    "WIN_FUNC = 'hann'\n",
    "N_MELS = 224\n",
    "F_MIN = 0\n",
    "F_MAX = SAMPLE_RATE / 2\n",
    "\n",
    "# Learning process:\n",
    "NAME_MODEL_0 = \"model_vgg16_inst.h5\"\n",
    "NAME_MODEL_0_PIC = 'model_vgg16_pic.png'\n",
    "NAME_MODEL_0_CHECKPOINT = 'model_vgg16_cp.ckpt'\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "N_CHANNELS = 3\n",
    "EPOCHS = 100\n",
    "CALL_BACKS = [tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=NAME_MODEL_0_CHECKPOINT,\n",
    "    save_weights_only=True,\n",
    "    verbose=0\n",
    ")]\n",
    "\n",
    "train_metadata = pd.read_csv('./train_metadata.csv')\n",
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e4b1477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load work classes:\n",
    "with open('./scored_birds.json', 'r') as f:\n",
    "    valid_classes = json.load(f)\n",
    "\n",
    "primary_labels = train_metadata.primary_label\n",
    "\n",
    "# Encode labels:\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "labels = encoder.fit_transform(primary_labels)\n",
    "labels = np.uint8(labels)\n",
    "\n",
    "NUM_CLASSES = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94395bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144843 files belonging to 152 classes.\n",
      "Using 115875 files for training.\n",
      "Found 144843 files belonging to 152 classes.\n",
      "Using 28968 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Make a dataset containing the training spectrograms\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VAL_SIZE,\n",
    "    directory=IMAGES_DIR,\n",
    "    shuffle=True,\n",
    "    color_mode='rgb',\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    subset=\"training\",\n",
    "    label_mode='categorical',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Make a dataset containing the validation spectrogram\n",
    "valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VAL_SIZE,\n",
    "    directory=IMAGES_DIR,\n",
    "    shuffle=True,\n",
    "    color_mode='rgb',\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    subset=\"validation\",\n",
    "    label_mode='categorical',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91fcda8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare our datasets for modelling\n",
    "def prepare(ds, augment=False):\n",
    "    # Define our one transformation\n",
    "    rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1./255)])\n",
    "    flip_and_rotate = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "    ])\n",
    "    \n",
    "    # Apply rescale to both datasets and augmentation only to training\n",
    "    ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n",
    "    if augment: ds = ds.map(lambda x, y: (flip_and_rotate(x, training=True), y))\n",
    "    return ds\n",
    "\n",
    "train_dataset = prepare(train_dataset, augment=False)\n",
    "valid_dataset = prepare(valid_dataset, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13111b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    from keras.layers import Input\n",
    "    vgg16_input = Input(shape = (224, 224, 3), name = 'Image_input')\n",
    "\n",
    "\n",
    "    ## The VGG model\n",
    "\n",
    "    from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "    #Get back the convolutional part of a VGG network trained on ImageNet\n",
    "    model_vgg16_conv = VGG16(weights='imagenet', include_top=False, input_tensor = vgg16_input)\n",
    "    # model_vgg16_conv.summary()\n",
    "\n",
    "   #Use the generated model \n",
    "    from keras.models import Model\n",
    "\n",
    "\n",
    "    output_vgg16_conv = model_vgg16_conv(vgg16_input)\n",
    "\n",
    "    #Add the fully-connected layers \n",
    "\n",
    "    x = Flatten(name='flatten')(output_vgg16_conv)\n",
    "    x = Dense(152, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    vgg16_pretrained = Model(vgg16_input,x)\n",
    "    # vgg16_pretrained.summary()\n",
    "\n",
    "    # Compile CNN model\n",
    "    sgd = tf.keras.optimizers.SGD(lr = 0.001)\n",
    "    vgg16_pretrained.compile(loss='categorical_crossentropy',optimizer = sgd,metrics=['accuracy'])\n",
    "\n",
    "    return vgg16_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea67aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotMetrics(history):\n",
    "    metrics = list()\n",
    "    for key, value in history.history.items():\n",
    "        metrics.append(key)\n",
    "        \n",
    "    for i in range(int(len(metrics) / 2)):\n",
    "        plt.figure(figsize=(24, 6))\n",
    "        plt.plot(history.history[metrics[i]], c =\"darkblue\")\n",
    "        plt.plot(history.history[metrics[i + int(len(metrics) / 2)]], c =\"crimson\")\n",
    "        plt.legend([\"Train\", \"Validation\"])\n",
    "        plt.title(\"Model\" + metrics[i])\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(metrics[i])\n",
    "        plt.grid(True, alpha = 0.2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41acc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import image \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984aedb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamroua/.local/lib/python3.6/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3622/3622 [==============================] - 29194s 8s/step - loss: 2.4773 - accuracy: 0.4397 - val_loss: 2.2220 - val_accuracy: 0.4776\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.22195, saving model to vgg16_weights.hdf5\n",
      "Epoch 2/100\n",
      "3622/3622 [==============================] - 26791s 7s/step - loss: 1.5721 - accuracy: 0.6252 - val_loss: 1.5762 - val_accuracy: 0.6298\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.22195 to 1.57616, saving model to vgg16_weights.hdf5\n",
      "Epoch 3/100\n",
      "3622/3622 [==============================] - 35538s 10s/step - loss: 1.2189 - accuracy: 0.7035 - val_loss: 1.5908 - val_accuracy: 0.6317\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.57616\n",
      "Epoch 4/100\n",
      "3622/3622 [==============================] - 37107s 10s/step - loss: 0.9769 - accuracy: 0.7582 - val_loss: 1.4188 - val_accuracy: 0.6657\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.57616 to 1.41879, saving model to vgg16_weights.hdf5\n",
      "Epoch 5/100\n",
      "3622/3622 [==============================] - 37375s 10s/step - loss: 0.7874 - accuracy: 0.8003 - val_loss: 1.6371 - val_accuracy: 0.6407\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.41879\n",
      "Epoch 6/100\n",
      "3622/3622 [==============================] - 36366s 10s/step - loss: 0.6275 - accuracy: 0.8370 - val_loss: 1.2372 - val_accuracy: 0.7259\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.41879 to 1.23722, saving model to vgg16_weights.hdf5\n",
      "Epoch 7/100\n",
      "3622/3622 [==============================] - 35611s 10s/step - loss: 0.4987 - accuracy: 0.8680 - val_loss: 1.3861 - val_accuracy: 0.7043\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.23722\n",
      "Epoch 8/100\n",
      "1814/3622 [==============>...............] - ETA: 4:51:43 - loss: 0.4199 - accuracy: 0.8877"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,LearningRateScheduler\n",
    "import math\n",
    "checkpointer = ModelCheckpoint('vgg16_weights.hdf5', verbose=1, save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='accuracy', patience=7, verbose=1)\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "        math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "\n",
    "model = getModel()\n",
    "tf.keras.utils.plot_model(model, NAME_MODEL_0_PIC, show_shapes=True)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[checkpointer, earlystopper,lrate]\n",
    ")\n",
    "model.save(NAME_MODEL_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5dd66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
