{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a81be900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchlibrosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6037caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/core/errors.py:144: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/home/hamroua/.local/lib/python3.6/site-packages/resampy/interpn.py:114: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
      "  _resample_loop_p(x, t_out, interp_win, interp_delta, num_table, scale, y)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01bafc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'flight call']</td>\n",
       "      <td>12.3910</td>\n",
       "      <td>-1.4930</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>08:00</td>\n",
       "      <td>https://www.xeno-canto.org/125458</td>\n",
       "      <td>afrsil1/XC125458.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>['houspa', 'redava', 'zebdov']</td>\n",
       "      <td>['call']</td>\n",
       "      <td>19.8801</td>\n",
       "      <td>-155.7254</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Dan Lane</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>08:30</td>\n",
       "      <td>https://www.xeno-canto.org/175522</td>\n",
       "      <td>afrsil1/XC175522.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'song']</td>\n",
       "      <td>16.2901</td>\n",
       "      <td>-16.0321</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:30</td>\n",
       "      <td>https://www.xeno-canto.org/177993</td>\n",
       "      <td>afrsil1/XC177993.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['alarm call', 'call']</td>\n",
       "      <td>17.0922</td>\n",
       "      <td>54.2958</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Oscar Campbell</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:00</td>\n",
       "      <td>https://www.xeno-canto.org/205893</td>\n",
       "      <td>afrsil1/XC205893.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['flight call']</td>\n",
       "      <td>21.4581</td>\n",
       "      <td>-157.7252</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Ross Gallardy</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16:30</td>\n",
       "      <td>https://www.xeno-canto.org/207431</td>\n",
       "      <td>afrsil1/XC207431.ogg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label                secondary_labels                     type  \\\n",
       "0       afrsil1                              []  ['call', 'flight call']   \n",
       "1       afrsil1  ['houspa', 'redava', 'zebdov']                 ['call']   \n",
       "2       afrsil1                              []         ['call', 'song']   \n",
       "3       afrsil1                              []   ['alarm call', 'call']   \n",
       "4       afrsil1                              []          ['flight call']   \n",
       "\n",
       "   latitude  longitude  scientific_name         common_name          author  \\\n",
       "0   12.3910    -1.4930  Euodice cantans  African Silverbill       Bram Piot   \n",
       "1   19.8801  -155.7254  Euodice cantans  African Silverbill        Dan Lane   \n",
       "2   16.2901   -16.0321  Euodice cantans  African Silverbill       Bram Piot   \n",
       "3   17.0922    54.2958  Euodice cantans  African Silverbill  Oscar Campbell   \n",
       "4   21.4581  -157.7252  Euodice cantans  African Silverbill   Ross Gallardy   \n",
       "\n",
       "                                             license  rating   time  \\\n",
       "0  Creative Commons Attribution-NonCommercial-Sha...     2.5  08:00   \n",
       "1  Creative Commons Attribution-NonCommercial-Sha...     3.5  08:30   \n",
       "2  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:30   \n",
       "3  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:00   \n",
       "4  Creative Commons Attribution-NonCommercial-Sha...     3.0  16:30   \n",
       "\n",
       "                                 url              filename  \n",
       "0  https://www.xeno-canto.org/125458  afrsil1/XC125458.ogg  \n",
       "1  https://www.xeno-canto.org/175522  afrsil1/XC175522.ogg  \n",
       "2  https://www.xeno-canto.org/177993  afrsil1/XC177993.ogg  \n",
       "3  https://www.xeno-canto.org/205893  afrsil1/XC205893.ogg  \n",
       "4  https://www.xeno-canto.org/207431  afrsil1/XC207431.ogg  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Data:\n",
    "TRAIN_DIR = './train_audio/'\n",
    "IMAGES_DIR = './images/'\n",
    "SAMPLE_RATE = 32000\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "# Data processing:\n",
    "N_FFT = 2048\n",
    "HOP_LEN = 512\n",
    "WIN_FUNC = 'hann'\n",
    "N_MELS = 224\n",
    "F_MIN = 0\n",
    "F_MAX = SAMPLE_RATE / 2\n",
    "\n",
    "# Learning process:\n",
    "NAME_MODEL_0 = \"model_xception_inst.h5\"\n",
    "NAME_MODEL_0_PIC = 'model_xception_pic.png'\n",
    "NAME_MODEL_0_CHECKPOINT = 'model_xception_cp.ckpt'\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "N_CHANNELS = 3\n",
    "EPOCHS = 100\n",
    "CALL_BACKS = [tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=NAME_MODEL_0_CHECKPOINT,\n",
    "    save_weights_only=True,\n",
    "    verbose=0\n",
    ")]\n",
    "\n",
    "train_metadata = pd.read_csv('./train_metadata.csv')\n",
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef2e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load work classes:\n",
    "with open('./scored_birds.json', 'r') as f:\n",
    "    valid_classes = json.load(f)\n",
    "\n",
    "primary_labels = train_metadata.primary_label\n",
    "\n",
    "# Encode labels:\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "labels = encoder.fit_transform(primary_labels)\n",
    "labels = np.uint8(labels)\n",
    "\n",
    "NUM_CLASSES = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25986f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144843 files belonging to 152 classes.\n",
      "Using 115875 files for training.\n",
      "Found 144843 files belonging to 152 classes.\n",
      "Using 28968 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Make a dataset containing the training spectrograms\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VAL_SIZE,\n",
    "    directory=IMAGES_DIR,\n",
    "    shuffle=True,\n",
    "    color_mode='rgb',\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    subset=\"training\",\n",
    "    label_mode='categorical',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Make a dataset containing the validation spectrogram\n",
    "valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VAL_SIZE,\n",
    "    directory=IMAGES_DIR,\n",
    "    shuffle=True,\n",
    "    color_mode='rgb',\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    subset=\"validation\",\n",
    "    label_mode='categorical',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c7220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare our datasets for modelling\n",
    "def prepare(ds, augment=False):\n",
    "    # Define our one transformation\n",
    "    rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1./255)])\n",
    "    flip_and_rotate = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "    ])\n",
    "    \n",
    "    # Apply rescale to both datasets and augmentation only to training\n",
    "    ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n",
    "    if augment: ds = ds.map(lambda x, y: (flip_and_rotate(x, training=True), y))\n",
    "    return ds\n",
    "\n",
    "train_dataset = prepare(train_dataset, augment=False)\n",
    "valid_dataset = prepare(valid_dataset, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d50815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import preprocess_input, decode_predictions\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def getModel():\n",
    "    xception_input = Input(shape = (224, 224, 3), name = 'Image_input')\n",
    "\n",
    "\n",
    "    ## The VGG model\n",
    "\n",
    "\n",
    "    #Get back the convolutional part of a VGG network trained on ImageNet\n",
    "    model_xception_conv = Xception(weights= 'imagenet', include_top=False, input_shape= (224,224,3))\n",
    "    # model_vgg16_conv.summary()\n",
    "\n",
    "   #Use the generated model \n",
    "\n",
    "\n",
    "    output_xception_conv = model_xception_conv(xception_input)\n",
    "\n",
    "\n",
    "    x = Flatten(name='flatten')(output_xception_conv)\n",
    "\n",
    "    x = Dense(152, activation='softmax', name='predictions')(x)\n",
    "\n",
    "\n",
    "    xception_pretrained = Model(xception_input, x)\n",
    "\n",
    "\n",
    "    adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0)\n",
    "    sgd = tf.keras.optimizers.SGD(lr = 0.001)\n",
    "    xception_pretrained.compile(loss='categorical_crossentropy',optimizer = sgd,metrics=['accuracy'])\n",
    "\n",
    "    return xception_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b60154a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotMetrics(history):\n",
    "    metrics = list()\n",
    "    for key, value in history.history.items():\n",
    "        metrics.append(key)\n",
    "        \n",
    "    for i in range(int(len(metrics) / 2)):\n",
    "        plt.figure(figsize=(24, 6))\n",
    "        plt.plot(history.history[metrics[i]], c =\"darkblue\")\n",
    "        plt.plot(history.history[metrics[i + int(len(metrics) / 2)]], c =\"crimson\")\n",
    "        plt.legend([\"Train\", \"Validation\"])\n",
    "        plt.title(\"Model\" + metrics[i])\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(metrics[i])\n",
    "        plt.grid(True, alpha = 0.2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c30dbc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import image \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7db2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamroua/.local/lib/python3.6/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3622/3622 [==============================] - 34104s 9s/step - loss: 2.8920 - accuracy: 0.3609 - val_loss: 2.3236 - val_accuracy: 0.4579\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.32361, saving model to xceptionm_weights.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamroua/.local/lib/python3.6/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "3622/3622 [==============================] - 40851s 11s/step - loss: 1.9324 - accuracy: 0.5472 - val_loss: 1.8848 - val_accuracy: 0.5536\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.32361 to 1.88484, saving model to xceptionm_weights.hdf5\n",
      "Epoch 3/100\n",
      "3622/3622 [==============================] - 38247s 11s/step - loss: 1.5281 - accuracy: 0.6350 - val_loss: 1.6767 - val_accuracy: 0.5997\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.88484 to 1.67672, saving model to xceptionm_weights.hdf5\n",
      "Epoch 4/100\n",
      "3622/3622 [==============================] - 41072s 11s/step - loss: 1.2668 - accuracy: 0.6933 - val_loss: 1.5155 - val_accuracy: 0.6392\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.67672 to 1.51553, saving model to xceptionm_weights.hdf5\n",
      "Epoch 5/100\n",
      "3622/3622 [==============================] - 37298s 10s/step - loss: 1.0739 - accuracy: 0.7396 - val_loss: 1.4289 - val_accuracy: 0.6574\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.51553 to 1.42888, saving model to xceptionm_weights.hdf5\n",
      "Epoch 6/100\n",
      "3622/3622 [==============================] - 39672s 11s/step - loss: 0.9210 - accuracy: 0.7770 - val_loss: 1.3423 - val_accuracy: 0.6797\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.42888 to 1.34225, saving model to xceptionm_weights.hdf5\n",
      "Epoch 7/100\n",
      "3622/3622 [==============================] - 46488s 13s/step - loss: 0.7931 - accuracy: 0.8092 - val_loss: 1.2868 - val_accuracy: 0.6899\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.34225 to 1.28678, saving model to xceptionm_weights.hdf5\n",
      "Epoch 8/100\n",
      "3622/3622 [==============================] - 44659s 12s/step - loss: 0.6878 - accuracy: 0.8368 - val_loss: 1.2543 - val_accuracy: 0.7014\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.28678 to 1.25432, saving model to xceptionm_weights.hdf5\n",
      "Epoch 9/100\n",
      "3622/3622 [==============================] - 30732s 8s/step - loss: 0.5975 - accuracy: 0.8595 - val_loss: 1.3002 - val_accuracy: 0.6889\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.25432\n",
      "Epoch 10/100\n",
      "3622/3622 [==============================] - 27820s 8s/step - loss: 0.5048 - accuracy: 0.8882 - val_loss: 1.2176 - val_accuracy: 0.7099\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.25432 to 1.21757, saving model to xceptionm_weights.hdf5\n",
      "Epoch 11/100\n",
      "3622/3622 [==============================] - 11219s 3s/step - loss: 0.4703 - accuracy: 0.8960 - val_loss: 1.2013 - val_accuracy: 0.7150\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.21757 to 1.20128, saving model to xceptionm_weights.hdf5\n",
      "Epoch 12/100\n",
      "3622/3622 [==============================] - 10775s 3s/step - loss: 0.4360 - accuracy: 0.9062 - val_loss: 1.1953 - val_accuracy: 0.7170\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.20128 to 1.19528, saving model to xceptionm_weights.hdf5\n",
      "Epoch 13/100\n",
      "3622/3622 [==============================] - 10853s 3s/step - loss: 0.4059 - accuracy: 0.9145 - val_loss: 1.1837 - val_accuracy: 0.7195\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.19528 to 1.18371, saving model to xceptionm_weights.hdf5\n",
      "Epoch 14/100\n",
      "3622/3622 [==============================] - 10846s 3s/step - loss: 0.3785 - accuracy: 0.9222 - val_loss: 1.1811 - val_accuracy: 0.7221\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.18371 to 1.18109, saving model to xceptionm_weights.hdf5\n",
      "Epoch 15/100\n",
      "3622/3622 [==============================] - 10807s 3s/step - loss: 0.3551 - accuracy: 0.9279 - val_loss: 1.1773 - val_accuracy: 0.7229\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.18109 to 1.17725, saving model to xceptionm_weights.hdf5\n",
      "Epoch 16/100\n",
      "  73/3622 [..............................] - ETA: 2:51:02 - loss: 0.3485 - accuracy: 0.9285"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,LearningRateScheduler\n",
    "import math\n",
    "checkpointer = ModelCheckpoint('xception_weights.hdf5', verbose=1, save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='accuracy', patience=7, verbose=1)\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "        math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "\n",
    "model = getModel()\n",
    "tf.keras.utils.plot_model(model, NAME_MODEL_0_PIC, show_shapes=True)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[checkpointer, earlystopper,lrate]\n",
    ")\n",
    "model.save(NAME_MODEL_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f36f6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
